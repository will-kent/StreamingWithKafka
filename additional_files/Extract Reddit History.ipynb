{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c7f9bc",
   "metadata": {},
   "source": [
    "# Extract Historical Reddit Data\n",
    "Data is extracted to aid ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e90b9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import calendar\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "0a0b34aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src/configuration/config.cfg']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read passwords and secrets from config file\n",
    "config_parser = configparser.ConfigParser()\n",
    "config_parser.read(\"src/configuration/config.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "a7743225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "sub_reddit = config_parser[\"praw\"][\"subreddit\"]\n",
    "sub_file_prefix = \"submissions\"\n",
    "comment_file_prefix = \"comment\"\n",
    "years = [2021]\n",
    "months = range(1,6)\n",
    "sub_columns = [\"id\",'author_fullname','title','score','author_premium','domain','over_18','subreddit_id','permalink','parent_whitelist_status','url','created_utc','num_comments','upvote_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ab0161ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(object_type, columns, username='', subreddit='', search_query='', max_time=None, min_time=1609459200):\n",
    "    # Start from current time if not specified\n",
    "    if max_time is None:\n",
    "        max_time = int(time.time())\n",
    "\n",
    "    # Generate filter string\n",
    "    filter_string = urllib.parse.urlencode(\n",
    "        {k: v for k, v in zip(\n",
    "            ['author', 'subreddit', 'q'],\n",
    "            [username, subreddit, search_query]) if v != \"\"})\n",
    "\n",
    "    url_format = \"https://api.pushshift.io/reddit/search/{}/?size=500&sort=desc&{}&before={}\"\n",
    "\n",
    "    before = max_time\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through period to retrieve all records\n",
    "    while before > min_time:\n",
    "        # Format the Url with variables and make the request\n",
    "        url = url_format.format(object_type, filter_string, before)\n",
    "        resp = requests.get(url)\n",
    "\n",
    "        # Convert records to dataframe\n",
    "        dfi = pd.json_normalize(json.loads(resp.text)['data'])\n",
    "        # Filter out unwanted columns\n",
    "        df = pd.concat([df, dfi[columns]])\n",
    "\n",
    "        # set `before` to the earliest comment/post in the results next\n",
    "        # requests.get(...) we will only retrieve submissions/comments before\n",
    "        # the earliest that we already have, thus not fetching any duplicates\n",
    "        before = dfi['created_utc'].min()\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "265c695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_reddit_data(years, months, days, object_type, columns, subreddit, file_prefix):\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            for day in days:\n",
    "                ymd = year * 10000 + month * 100 + day\n",
    "                tomorrow = day + 1\n",
    "                \n",
    "                # Set start and end date for request\n",
    "                after_time = start_epoch=int(dt.datetime(year, month, day).timestamp())\n",
    "                before_time = start_epoch=int(dt.datetime(year, month, tomorrow).timestamp())\n",
    "                \n",
    "                # Get the required Subreddit data at the daily level\n",
    "                df = get_subreddit_data(\n",
    "                    object_type=object_type,\n",
    "                    columns=columns,\n",
    "                    username=username,\n",
    "                    subreddit=subreddit,\n",
    "                    max_time=before_time,\n",
    "                    min_time=after_time)\n",
    "\n",
    "                # Check for duplicates and retirve number of unique records\n",
    "                dupes = df['id'].duplicated().any()\n",
    "                total = df['id'].nunique() \n",
    "\n",
    "                if dupes:\n",
    "                    print(\"There are duplicates in the data for \" + str(ymd))\n",
    "\n",
    "                print(\"For \" + str(ymd) + \" \" + str(total) + \" values were extracted\")\n",
    "\n",
    "                filename = file_prefix + \"_\" + str(ymd) + \".csv\"\n",
    "            \n",
    "                # Save Data to a CSV\n",
    "                df.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "75cfb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_reddit_data(years, months, object_type, columns, subreddit, file_prefix):\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            ym = year * 100 + month\n",
    "            last_day = calendar.monthrange(year, month)[1]\n",
    "            \n",
    "            # Set start and end date for request\n",
    "            after_time = start_epoch=int(dt.datetime(year, month, 1).timestamp())\n",
    "            before_time = start_epoch=int(dt.datetime(year, month, last_day).timestamp())\n",
    "    \n",
    "            # Get the required Subreddit data at the monthly level\n",
    "            df = get_subreddit_data(\n",
    "                object_type=object_type,\n",
    "                username=username,\n",
    "                columns=columns,\n",
    "                subreddit=subreddit,\n",
    "                max_time=before_time,\n",
    "                min_time=after_time)\n",
    "\n",
    "            # Check for duplicates and retirve number of unique records\n",
    "            dupes = df['id'].duplicated().any()\n",
    "            total = df['id'].nunique() \n",
    "            \n",
    "            if dupes:\n",
    "                print(\"There are duplicates in the data for \" + str(ymd))\n",
    "                      \n",
    "            print(\"For \" + str(ym) + \" \" + str(total) + \" values were extracted\")\n",
    "\n",
    "            filename = file_prefix + \"_\" + str(ym) + \".csv\"\n",
    "\n",
    "            # Save Data to a CSV\n",
    "            df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_reddit_data(years=years\n",
    "                      ,months=months\n",
    "                      ,object_type=\"submission\"\n",
    "                      ,columns=sub_columns\n",
    "                      ,subreddit=sub_reddit\n",
    "                      ,file_prefix=sub_file_prefix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f927037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
