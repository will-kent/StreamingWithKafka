{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0465c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stream predictions\n",
    "Using the random forest model created on historical data stream predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d57e8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark import broadcast, SparkContext\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, Tokenizer, RegexTokenizer, StopWordsRemover, OneHotEncoder, StringIndexer, VectorAssembler, VectorIndexer, Bucketizer\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, LocalLDAModel\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a3d2208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "broker = \"broker:29092\"\n",
    "num_topics = 20\n",
    "cat_cols = ['domain','hour','day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2a75298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('kafka') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7a13d",
   "metadata": {},
   "source": [
    "## Load the raw stream and convert to dataframe object for processing\n",
    "Consume data from the submission topic, prepare data for ML model and use ML model to predict number of comments on submissions as they are received. Predictions to feed back as a topic named **comment predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f53897b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw stream data for submissions and convert to df useable in processing to prediction\n",
    "# NOTE - no need to do this for comments stream as the model is pre-trained and only applied to submission data\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", broker) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"subscribe\", \"submissions\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "57b1200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_stream_df = df \\\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType())) \\\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "afac75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the structure of the value component\n",
    "schema_submissions = StructType([\n",
    "    StructField(\"id\", StringType(),  True),\n",
    "    StructField(\"author_fullname\", StringType(),  True),\n",
    "    StructField(\"title\", StringType(),  True),\n",
    "    StructField(\"subreddit_name_prefixed\", StringType(),  True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"upvote_ratio\", DoubleType(),  True),\n",
    "    StructField(\"ups\", IntegerType(), True),\n",
    "    StructField(\"created\", IntegerType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"url_overridden_by_dest\", StringType(), True),\n",
    "    StructField(\"over_18\", StringType(), True),\n",
    "    StructField(\"subreddit_id\", StringType(),  True),\n",
    "    StructField(\"permalink\", StringType(),  True),\n",
    "    StructField(\"parent_whitelist_status\", StringType(),  True),\n",
    "    StructField(\"url\", StringType(),  True),\n",
    "    StructField(\"created_utc\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "bfdf8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To json to split our the values in message\n",
    "json_stream_df = submission_stream_df.withColumn(\"value\", F.from_json(\"value\", schema_submissions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6d1fa520",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "\n",
    "# Define stopwords for removal from prediction\n",
    "stop_words =StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stop_words = stop_words + ['a','i']\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_words)\n",
    "\n",
    "# Load the vectorizer trained on the original training data and used for the LDA model training and hence the rf model relying on it.\n",
    "cvmodel = CountVectorizerModel.load('count_vectorizer_model')\n",
    "\n",
    "# Load the LDA model trained on the original training data \n",
    "lda_model = LocalLDAModel.load('lda_distributed_model')\n",
    "\n",
    "# Load the pre-trained random forest model\n",
    "pipeline_model = PipelineModel.load('pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c535e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the submissions dataframe, with predictions, to be streamed\n",
    "submissions_stream_df = json_stream_df \\\n",
    "    .select( \\\n",
    "        F.col(\"key\").alias(\"event_key\"), \\\n",
    "        F.col(\"topic\").alias(\"event_topic\"), \\\n",
    "        F.col(\"timestamp\").alias(\"event_timestamp\"), \\\n",
    "            \"value.id\", \\\n",
    "            \"value.author_fullname\", \\\n",
    "            \"value.title\", \\\n",
    "            \"value.subreddit_name_prefixed\", \\\n",
    "            \"value.name\", \\\n",
    "            \"value.upvote_ratio\", \\\n",
    "            \"value.ups\", \\\n",
    "            \"value.created\", \\\n",
    "            \"value.domain\", \\\n",
    "            \"value.url_overridden_by_dest\", \\\n",
    "            \"value.over_18\", \\\n",
    "            \"value.subreddit_id\", \\\n",
    "            \"value.permalink\", \\\n",
    "            \"value.parent_whitelist_status\", \\\n",
    "            \"value.url\",\n",
    "            \"value.created_utc\"\n",
    "           )\n",
    "\n",
    "submissions_stream_df = submissions_stream_df.select('id','title','domain','subreddit_id','event_timestamp') \\\n",
    "    .withColumn(\"title\", F.regexp_replace(F.col(\"title\"), '[^\\sa-zA-Z]', '')) \\\n",
    "    .withColumn(\"hour\", F.hour(F.col(\"event_timestamp\"))).withColumn(\"day\", F.dayofweek(F.col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"hour\", F.col(\"hour\").astype(StringType())).withColumn(\"day\", F.col(\"day\").astype(StringType()))\n",
    "\n",
    "submissions_stream_df = tokenizer.transform(submissions_stream_df)\n",
    "submissions_stream_df = remover.transform(submissions_stream_df)\n",
    "submissions_stream_df = cvmodel.transform(submissions_stream_df)\n",
    "#submissions_stream_df = submissions_stream_df.select('vectors', 'id')\n",
    "submissions_stream_df = lda_model.transform(submissions_stream_df)\n",
    "\n",
    "submissions_stream_df = submissions_stream_df.withColumn(\"T_\", vector_to_array(\"topicDistribution\")) \\\n",
    "    .drop('vectors', 'topicDistribution') \\\n",
    "    .select([\"id\",\"domain\",\"hour\",\"day\"] + [F.col(f\"T_\")[i] for i in range(0,num_topics)]) \\\n",
    "    .withColumnRenamed(\"T_[0]\", \"T_1\") \\\n",
    "    .withColumnRenamed(\"T_[1]\", \"T_2\") \\\n",
    "    .withColumnRenamed(\"T_[2]\", \"T_3\") \\\n",
    "    .withColumnRenamed(\"T_[3]\", \"T_4\") \\\n",
    "    .withColumnRenamed(\"T_[4]\", \"T_5\") \\\n",
    "    .withColumnRenamed(\"T_[5]\", \"T_6\") \\\n",
    "    .withColumnRenamed(\"T_[6]\", \"T_7\") \\\n",
    "    .withColumnRenamed(\"T_[7]\", \"T_8\") \\\n",
    "    .withColumnRenamed(\"T_[8]\", \"T_9\") \\\n",
    "    .withColumnRenamed(\"T_[9]\", \"T_10\") \\\n",
    "    .withColumnRenamed(\"T_[10]\", \"T_11\") \\\n",
    "    .withColumnRenamed(\"T_[11]\", \"T_12\") \\\n",
    "    .withColumnRenamed(\"T_[12]\", \"T_13\") \\\n",
    "    .withColumnRenamed(\"T_[13]\", \"T_14\") \\\n",
    "    .withColumnRenamed(\"T_[14]\", \"T_15\") \\\n",
    "    .withColumnRenamed(\"T_[15]\", \"T_16\") \\\n",
    "    .withColumnRenamed(\"T_[16]\", \"T_17\") \\\n",
    "    .withColumnRenamed(\"T_[17]\", \"T_18\") \\\n",
    "    .withColumnRenamed(\"T_[18]\", \"T_19\") \\\n",
    "    .withColumnRenamed(\"T_[19]\", \"T_20\")\n",
    "\n",
    "# Apply model to new submissions\n",
    "submissions_stream_df = pipeline_model.transform(submissions_stream_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "73094899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction stream\n",
    "submissions_stream = submissions_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"submissions_view\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48d0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data in the stream using pyspark SQL\n",
    "subsmissions_data = spark.sql('SELECT * FROM submissions_view')\n",
    "print(subsmissions_data.count())\n",
    "subsmissions_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bb835a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f280616ab80>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stream data as a new topic\n",
    "submissions_stream_df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "   .writeStream \\\n",
    "   .format(\"kafka\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"kafka.bootstrap.servers\", broker) \\\n",
    "   .option(\"topic\", \"comment_predictions\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/HDFS/dir\") \\\n",
    "   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8bdd5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b90a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
