{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f8c6ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stream predictions\n",
    "Using the random forest model created on historical data stream predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c2fa1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "#import configparser\n",
    "#from confluent_kafka import Producer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import json\n",
    "#import logging\n",
    "#from multiprocessing import Process\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "#import praw\n",
    "import pyspark\n",
    "from pyspark import broadcast, SparkContext\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import CountVectorizer, CountVectorizerModel, Tokenizer, RegexTokenizer, StopWordsRemover, OneHotEncoder, StringIndexer, VectorAssembler, VectorIndexer, Bucketizer\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, LocalLDAModel\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "# from pyspark.sql import SQLContext\n",
    "import socket\n",
    "import re\n",
    "#import sys\n",
    "#import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0f4f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "broker = \"broker:29092\"\n",
    "num_topics = 20\n",
    "cat_cols = ['domain','hour','day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "62590e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('kafka') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad637a",
   "metadata": {},
   "source": [
    "## Load the raw stream and convert to dataframe object for processing\n",
    "Consume data from the submission topic, prepare data for ML model and use ML model to predict number of comments on submissions as they are received. Predictions to feed back as a topic named **comment predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2f123a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw stream data for submissions and convert to df useable in processing to prediction\n",
    "# NOTE - no need to do this for comments stream as the model is pre-trained and only applied to submission data\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"subscribe\", \"submissions\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c277ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_stream_df = df \\\n",
    "    .withColumn(\"key\", df[\"key\"].cast(StringType())) \\\n",
    "    .withColumn(\"value\", df[\"value\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "07323a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the structure of the value component\n",
    "schema_submissions = StructType([\n",
    "    StructField(\"id\", StringType(),  True),\n",
    "    StructField(\"author_fullname\", StringType(),  True),\n",
    "    StructField(\"title\", StringType(),  True),\n",
    "    StructField(\"subreddit_name_prefixed\", StringType(),  True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"upvote_ratio\", DoubleType(),  True),\n",
    "    StructField(\"ups\", IntegerType(), True),\n",
    "    StructField(\"created\", IntegerType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"url_overridden_by_dest\", StringType(), True),\n",
    "    StructField(\"over_18\", StringType(), True),\n",
    "    StructField(\"subreddit_id\", StringType(),  True),\n",
    "    StructField(\"permalink\", StringType(),  True),\n",
    "    StructField(\"parent_whitelist_status\", StringType(),  True),\n",
    "    StructField(\"url\", StringType(),  True),\n",
    "    StructField(\"created_utc\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3bb06df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To json to split our the values in message\n",
    "json_stream_df = submission_stream_df.withColumn(\"value\", F.from_json(\"value\", schema_submissions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c58acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "\n",
    "# Define stopwords for removal from prediction\n",
    "stop_words =StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stop_words = stop_words + ['a','i']\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_words)\n",
    "\n",
    "# Load the vectorizer trained on the original training data and used for the LDA model training and hence the rf model relying on it.\n",
    "cvmodel = CountVectorizerModel.load('count_vectorizer_model')\n",
    "\n",
    "# Load the LDA model trained on the original training data \n",
    "lda_model = LocalLDAModel.load('lda_distributed_model')\n",
    "\n",
    "# Load the pre-trained random forest model\n",
    "pipeline_model = PipelineModel.load('pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4b3fec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the submissions dataframe, with predictions, to be streamed\n",
    "submissions_stream_df = json_stream_df \\\n",
    "    .select( \\\n",
    "        F.col(\"key\").alias(\"event_key\"), \\\n",
    "        F.col(\"topic\").alias(\"event_topic\"), \\\n",
    "        F.col(\"timestamp\").alias(\"event_timestamp\"), \\\n",
    "            \"value.id\", \\\n",
    "            \"value.author_fullname\", \\\n",
    "            \"value.title\", \\\n",
    "            \"value.subreddit_name_prefixed\", \\\n",
    "            \"value.name\", \\\n",
    "            \"value.upvote_ratio\", \\\n",
    "            \"value.ups\", \\\n",
    "            \"value.created\", \\\n",
    "            \"value.domain\", \\\n",
    "            \"value.url_overridden_by_dest\", \\\n",
    "            \"value.over_18\", \\\n",
    "            \"value.subreddit_id\", \\\n",
    "            \"value.permalink\", \\\n",
    "            \"value.parent_whitelist_status\", \\\n",
    "            \"value.url\",\n",
    "            \"value.created_utc\"\n",
    "           )\n",
    "\n",
    "submissions_stream_df = submissions_stream_df.select('id','title','domain','subreddit_id','event_timestamp') \\\n",
    "    .withColumn(\"title\", F.regexp_replace(F.col(\"title\"), '[^\\sa-zA-Z]', '')) \\\n",
    "    .withColumn(\"hour\", F.hour(F.col(\"event_timestamp\"))).withColumn(\"day\", F.dayofweek(F.col(\"event_timestamp\"))) \\\n",
    "    .withColumn(\"hour\", F.col(\"hour\").astype(StringType())).withColumn(\"day\", F.col(\"day\").astype(StringType()))\n",
    "\n",
    "submissions_stream_df = tokenizer.transform(submissions_stream_df)\n",
    "submissions_stream_df = remover.transform(submissions_stream_df)\n",
    "submissions_stream_df = cvmodel.transform(submissions_stream_df)\n",
    "#submissions_stream_df = submissions_stream_df.select('vectors', 'id')\n",
    "submissions_stream_df = lda_model.transform(submissions_stream_df)\n",
    "\n",
    "submissions_stream_df = submissions_stream_df.withColumn(\"T_\", vector_to_array(\"topicDistribution\")) \\\n",
    "    .drop('vectors', 'topicDistribution') \\\n",
    "    .select([\"id\",\"domain\",\"hour\",\"day\"] + [F.col(f\"T_\")[i] for i in range(0,num_topics)]) \\\n",
    "    .withColumnRenamed(\"T_[0]\", \"T_1\") \\\n",
    "    .withColumnRenamed(\"T_[1]\", \"T_2\") \\\n",
    "    .withColumnRenamed(\"T_[2]\", \"T_3\") \\\n",
    "    .withColumnRenamed(\"T_[3]\", \"T_4\") \\\n",
    "    .withColumnRenamed(\"T_[4]\", \"T_5\") \\\n",
    "    .withColumnRenamed(\"T_[5]\", \"T_6\") \\\n",
    "    .withColumnRenamed(\"T_[6]\", \"T_7\") \\\n",
    "    .withColumnRenamed(\"T_[7]\", \"T_8\") \\\n",
    "    .withColumnRenamed(\"T_[8]\", \"T_9\") \\\n",
    "    .withColumnRenamed(\"T_[9]\", \"T_10\") \\\n",
    "    .withColumnRenamed(\"T_[10]\", \"T_11\") \\\n",
    "    .withColumnRenamed(\"T_[11]\", \"T_12\") \\\n",
    "    .withColumnRenamed(\"T_[12]\", \"T_13\") \\\n",
    "    .withColumnRenamed(\"T_[13]\", \"T_14\") \\\n",
    "    .withColumnRenamed(\"T_[14]\", \"T_15\") \\\n",
    "    .withColumnRenamed(\"T_[15]\", \"T_16\") \\\n",
    "    .withColumnRenamed(\"T_[16]\", \"T_17\") \\\n",
    "    .withColumnRenamed(\"T_[17]\", \"T_18\") \\\n",
    "    .withColumnRenamed(\"T_[18]\", \"T_19\") \\\n",
    "    .withColumnRenamed(\"T_[19]\", \"T_20\")\n",
    "\n",
    "# Apply model to new submissions\n",
    "submissions_stream_df = pipeline_model.transform(submissions_stream_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "388b3930",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name submissions_view as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-baa35aeb3c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create prediction stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msubmissions_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmissions_stream_df\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submissions_view\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name submissions_view as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "# Create prediction stream\n",
    "submissions_stream = submissions_stream_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"submissions_view\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5713416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+--------+---------------+-------+---------+--------------------+--------------------+--------------------+----------+\n",
      "|    id|       domain|hour|day|                 T_1|                 T_2|                 T_3|                 T_4|                 T_5|                 T_6|                 T_7|                 T_8|               T_9|                T_10|                T_11|                T_12|                T_13|                T_14|                T_15|                T_16|                T_17|                T_18|                T_19|                T_20|domain_ind|         domain_ohe|hour_ind|       hour_ohe|day_ind|  day_ohe|            features|       rawPrediction|         probability|prediction|\n",
      "+------+-------------+----+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+--------+---------------+-------+---------+--------------------+--------------------+--------------------+----------+\n",
      "|ny36rp|  tbsnews.net|   0|  1|0.006643685119029522|0.006642264299584...|0.006684056448681591|0.006646320633416965|0.006668571092706447|0.006646145782151997| 0.00666099204896445|0.006646412975283193|0.8720523613622571|0.006649968382010252| 0.00802360951748424|0.006759241216945...|0.006639764575637555|0.006682909793094758|0.006663223802852881|0.006649863093061475|0.006670091990183808|0.006643009959856153|0.006657136605325894|0.006670371301471226|     318.0| (5605,[318],[1.0])|    18.0|(23,[18],[1.0])|    6.0|(6,[],[])|(5654,[318,5623,5...|[16.3419777023567...|[0.81709888511783...|       0.0|\n",
      "|ny3hq4|bloomberg.com|   0|  1|0.004642999877756832|0.004642006925186437|0.004671213749339101|0.004644841731044...|0.004660391667997882|0.004644719534777897|0.004655094983693341|0.004644906263857361|0.6405508915773985|0.004647390993211223| 0.27563910800812624|0.004723757312752...|0.004640259969994354|0.004670412400517579|0.004656654664795...|0.004647317410540958|0.004661454561226606|0.004642528035962485|0.004652400571388547|0.004661649760432593|      39.0|  (5605,[39],[1.0])|    18.0|(23,[18],[1.0])|    6.0|(6,[],[])|(5654,[39,5623,56...|[16.3419777023567...|[0.81709888511783...|       0.0|\n",
      "|ny3j81|  reuters.com|   0|  1|0.005809273008716862|0.005808030644193467|0.005844573930716891|0.005811577506367363|0.005831033422540195|0.005811424647030277|0.005824406283225549|0.005811658257088477|0.8881219132859819|0.005814767127215679|0.007015888267455...|0.005910315898774034|0.005805844868164629|0.005843571335257852|0.005826357762664381|0.005814675146177504|0.005832363323756097|0.005808682671260743| 0.00582103506376467|0.005832607549647549|       1.0|   (5605,[1],[1.0])|    18.0|(23,[18],[1.0])|    6.0|(6,[],[])|(5654,[1,5623,563...|[16.3419777023567...|[0.81709888511783...|       0.0|\n",
      "|ny3o6f|    ohchr.org|   0|  1|0.002897742421111246|0.002897122709373...|0.002915350980963427|0.002898891939975034|0.002908596811114...|0.002898815676060...|0.002905291096569...|0.002898932215858074|0.9441937275541701|0.002900482959984465|0.003499617286627865|0.002948143937908518|0.002896032417511509|0.002914850847977...|0.002906264510035601|0.002900437036756904|0.002909260171937...|0.002897447940741395|0.002903609487561...|0.002909381997762113|    1435.0|(5605,[1435],[1.0])|    18.0|(23,[18],[1.0])|    6.0|(6,[],[])|(5654,[1435,5623,...|[16.3419777023567...|[0.81709888511783...|       0.0|\n",
      "|ny3usy|  m.jpost.com|   0|  1|0.007758002066327758|0.007756342948234284|0.007805144712894...|0.007761079625997717|0.007787062066586336|0.007760875452103...|0.007778211819395097| 0.00776118746110946|0.8505922500628471|0.007765339194572507|0.009369375154157884|0.007892939918761617|0.007753423983811722|0.007803805731492876|0.007780817894083245|0.007765216247345...|0.007788838056560996|0.007757213672255136|0.007773709717896643|0.007789164213566035|     125.0| (5605,[125],[1.0])|    18.0|(23,[18],[1.0])|    6.0|(6,[],[])|(5654,[125,5623,5...|[17.3667148331489...|[0.86833574165744...|       0.0|\n",
      "+------+-------------+----+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+--------+---------------+-------+---------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "# View data in the stream using pyspark SQL\n",
    "subsmissions_data = spark.sql('SELECT * FROM submissions_view')\n",
    "subsmissions_data.show(5)\n",
    "print(subsmissions_data.count())\n",
    "# subs_data.show(5, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7fe1d2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f2806193520>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stream data as a new topic\n",
    "submissions_stream_df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "   .writeStream \\\n",
    "   .format(\"kafka\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "   .option(\"topic\", \"comment_predictions\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/HDFS/dir\") \\\n",
    "   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c631a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db53e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
